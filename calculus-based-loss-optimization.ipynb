{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed44aed",
   "metadata": {},
   "source": [
    "Objectives\n",
    "\n",
    "    Understand the difference between closed-form and iterative optimization\n",
    "    Implement basic gradient descent by hand and in Python\n",
    "    Visualize how optimization algorithms converge to minima\n",
    "\n",
    "Part 1: Hand Calculations (50 points)\n",
    "Problem 1.1: Closed-Form Solution (10 points)\n",
    "\n",
    "Find the minimum of f(x) = x² - 4x + 7 using calculus (closed-form solution).\n",
    "\n",
    "Hints:\n",
    "\n",
    "    Find f'(x) and solve f'(x) = 0\n",
    "\n",
    "Problem 1.2: Gradient Descent by Hand (20 points)\n",
    "\n",
    "Use gradient descent to find the minimum of the same function f(x) = x² - 4x + 7.\n",
    "\n",
    "Starting from x₀ = 0 with learning rate α = 0.1:\n",
    "\n",
    "    Perform 5 iterations of gradient descent by hand\n",
    "    Create a table showing: iteration, x_n, f'(x_n), f(x_n)\n",
    "    How close did you get to the true minimum after 5 iterations?\n",
    "    The sign of f'(x_n) tells the direction to move. Why is the magnitude of the value of f'(x_n) used as the step size?\n",
    "\n",
    "Hints:\n",
    "\n",
    "    Use the update rule: x_{n+1} = x_n - α × f'(x_n)\n",
    "\n",
    "Problem 1.3: Introducing Cost Functions - Fence Optimization (20 points)\n",
    "\n",
    "A farmer wants to build a rectangular fence to enclose exactly 1 acre of land (43,560 ft²). Fencing costs $8 per foot. The farmer wants to minimize the total fencing cost.\n",
    "\n",
    "Since the area must be exactly 1 acre, if we choose the length L in feet, then the width W is determined by: W = 43,560/L\n",
    "Derive the Cost Function \n",
    "\n",
    "    Write an expression for the perimeter of the rectangle in terms of length L only\n",
    "    Write the cost function C(L) that gives the total fencing cost in dollars as a function of length L\n",
    "    What is the domain of this function? (What range of values of L make sense?)\n",
    "\n",
    "Find the Minimum Cost (Closed-Form) \n",
    "\n",
    "    Find C'(L) and solve C'(L) = 0 to find the optimal length\n",
    "    What is the corresponding optimal width?\n",
    "    What is the minimum total cost?\n",
    "    What do you notice about the relationship between the optimal length and width?\n",
    "\n",
    "Note for Students: This introduces the idea of a cost or loss function that we want to minimize. Here, our cost function C(L) represents the total expense we want to minimize, subject to the constraint that we must enclose exactly 1 acre. In machine learning, we'll minimize functions that represent how \"bad\" our model's predictions are. As the model learns, the predictions will get less bad.\n",
    "Part 2: Python Implementation (50 points)\n",
    "Problem 2.1: Implement Gradient Descent (25 points)\n",
    "\n",
    "For the following problem, use the given base function stump and extend it to accomplish the required functionality. Do not change the inputs or the return statement.\n",
    "\n",
    "Don't use an LLM! Homework is graded based on a good faith effort (not for correctness) so there's no value to getting a perfect answer from an LLM. The goal of this is for you to gain personal understanding of how to code gradient descent. Using an LLM will hurt you because you will be expected to write this code (or very similar) on a test/quiz.\n",
    "\n",
    "def function1(x):\n",
    "    return x**2 -4*x + 7\n",
    "\n",
    "def derivative1(x):\n",
    "    return ...\n",
    "\n",
    "def gradient_descent(f, df, x_start, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    f: function to minimize\n",
    "    df: derivative of f\n",
    "    x_start: starting point\n",
    "    learning_rate: step size\n",
    "    num_iterations: number of iterations to run\n",
    "    \n",
    "    Returns: (x_history, f_history) - lists of x values and f(x) values\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    # delete this and the following line in your implementation. (do not delete the return)\n",
    "    pass\n",
    "    return x_history, f_history\n",
    "\n",
    "# Example call of gradient descent.\n",
    "# gradient_descent(function1, derivative1, 0.01, 100)\n",
    "\n",
    "Test your function on f(x) = x² - 4x + 7 with:\n",
    "\n",
    "    Starting point: x₀ = 0\n",
    "    Learning rate: α = 0.1\n",
    "    50 iterations\n",
    "\n",
    "Part 3: Analysis and Visualization (20 points)\n",
    "Problem 3.1: Convergence Visualization\n",
    "\n",
    "Create two plots using matplotlib:\n",
    "\n",
    "    Function Plot: Plot the cost function of the fence from question 1.3 for lengths from l = 100 to l = 21500, with the minimum point clearly marked\n",
    "    Convergence Plot: For α = 0.1, use your gradient descent function to show how x_n approaches the minimum over 1000 iterations with a convergence plot (x-axis: iteration, y-axis: cost). \n",
    "    How many iterations did it take to get within 0.01 of the true minimum with α = 0.1?\n",
    "\n",
    "Problem 3.2: Reflection Questions\n",
    "\n",
    "    The closed form derivative optimization solution is obviously much faster than an iterative approach. When might iterative methods be necessary?\n",
    "    What role does the learning rate play in convergence speed and stability?\n",
    "\n",
    "Submission Requirements\n",
    "\n",
    "    A single PDF containing\n",
    "        Hand calculations (can be scanned/photographed if handwritten)\n",
    "        Answers to any questions\n",
    "        Python code \n",
    "        Plots (save as .png files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cf4e03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install uv --quiet\n",
    "%uv pip install numpy pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42987d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ffc7e",
   "metadata": {},
   "source": [
    "## f(x_n+1) = f(x_n) - (learning_rate * f'(x_n))\n",
    "## for n in len(range(num_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1aa837e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(x):\n",
    "    return x**2 -4*x + 7\n",
    "\n",
    "def derivative1(x):\n",
    "    return 2*x - 4\n",
    "\n",
    "def gradient_descent(f, df, x_start, learning_rate, num_iterations):\n",
    "    \"\"\"\n",
    "    f: function to minimize\n",
    "    df: derivative of f\n",
    "    x_start: starting point\n",
    "    learning_rate: step size\n",
    "    num_iterations: number of iterations to run\n",
    "    \n",
    "    Returns: (x_history, f_history) - lists of x values and f(x) values\n",
    "    \"\"\"\n",
    "\n",
    "    x = x_start\n",
    "    x_history = [x]\n",
    "    f_history = [f(x)]\n",
    "    for n in range(num_iterations):\n",
    "        x = x - learning_rate * df(x)\n",
    "        x_history.append(x)\n",
    "        f_history.append(f(x))\n",
    "        \n",
    "\n",
    "    return x_history, f_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90ac8d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 0.4, 0.7200000000000001, 0.976, 1.1808, 1.34464], [7, 5.5600000000000005, 4.6384, 4.048576000000001, 3.67108864, 3.4294967296])\n"
     ]
    }
   ],
   "source": [
    "results = gradient_descent(function1, derivative1, 0, 0.10, 5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be3908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb536d60",
   "metadata": {},
   "source": [
    "Problem 2.2: Explore Different Learning Rates (25 points)\n",
    "\n",
    "Using your gradient_descent function, test the following learning rates on function 2:\n",
    "\n",
    "    function2(x) = (x-1)² - 10x + 3 \n",
    "    α = 0.01, 0.1, 0.5, 0.9\n",
    "\n",
    "For each learning rate:\n",
    "\n",
    "    Run for 100 iterations\n",
    "    Plot the convergence using matplotlib (x-axis: iteration, y-axis: f(x) value)\n",
    "        Matplotlib quick reference here\n",
    "    Report the final x value and f(x) value\n",
    "\n",
    "Questions to answer:\n",
    "\n",
    "    Which learning rate converges fastest?\n",
    "    What happens with α = 0.9? Why?\n",
    "    What would you expect with α = 1.1? (Don't implement, just reason about it)\n",
    "    How many iterations did it take to get within 0.01 of the true minimum with α = 0.1\n",
    "\n",
    "Hints:\n",
    "\n",
    "    Expand and simplify function 2 to find its derivative more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f6b96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2(x):\n",
    "    return (x-1)**2 - 10*x + 3\n",
    "\n",
    "def derivative2(x):\n",
    "    return 2*x -12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e6deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 learning rate.  ([0, 0.12, 0.23759999999999998, 0.352848, 0.46579104, 0.5764752192, 0.684945714816, 0.79124680051968, 0.8954218645092864, 0.9975134272191006, 1.0975631586747185, 1.1956118955012243, 1.2916996575911999, 1.3858656644393759, 1.4781483511505884, 1.5685853841275765, 1.657213676445025, 1.7440694029161243, 1.829188014857802, 1.9126042545606459, 1.9943521694694328, 2.074465126080044, 2.1529758235584433, 2.2299163070872745, 2.305317980945529, 2.3792116213266183, 2.451627388900086, 2.522594841122084, 2.5921429442996424, 2.6603000854136494, 2.7270940837053765, 2.792552202031269, 2.856701157990644, 2.9195671348308307, 2.981175792134214, 3.04155227629153, 3.1007212307656995, 3.1587068061503856, 3.215532670027378, 3.2712220166268304, 3.3257975762942937, 3.379281624768408, 3.4316959922730397, 3.483062072427579, 3.5334008309790272, 3.5827328143594466, 3.6310781580722575, 3.6784565949108123, 3.724887463012596, 3.770389713752344, 3.8149819194772974, 3.8586822810877512, 3.901508635465996, 3.943478462756676, 3.9846088935015422, 4.0249167156315115, 4.064418381318881, 4.103130013692503, 4.141067413418654, 4.17824606515028, 4.214681143847275, 4.25038752097033, 4.285379770550923, 4.319672175139905, 4.353278731637107, 4.386213157004365, 4.418488893864278, 4.450119115986992, 4.481116733667252, 4.511494398993907, 4.541264511014029, 4.570439220793749, 4.599030436377874, 4.627049827650317, 4.65450883109731, 4.6814186544753635, 4.707790281385856, 4.733634475758139, 4.758961786242976, 4.783782550518116, 4.808106899507754, 4.831944761517599, 4.855305866287247, 4.878199748961502, 4.9006357539822725, 4.922623038902627, 4.944170578124574, 4.965287166562082, 4.98598142323084, 5.006261794766224, 5.026136558870899, 5.045613827693481, 5.064701551139612, 5.083407520116819, 5.101739369714482, 5.1197045823201925, 5.137310490673789, 5.154564280860313, 5.171472995243107, 5.188043535338245, 5.204282664631481], [4, 2.5744, 1.2052537600000002, -0.10967428889600006, -1.372531187055718, -2.5853789520483117, -3.7501979455471988, -4.86889010690353, -5.94328205867015, -6.975128089146812, -7.966113016816598, -8.917854941350662, -9.831907885673175, -10.709764333400516, -11.552857665797857, -12.362564502232262, -13.140206947943863, -13.887054752805284, -14.604327384594196, -15.293196020164267, -15.954785457765762, -16.590175953638237, -17.20040498587416, -17.786468948433544, -18.349324778075577, -18.889891516863784, -19.40905181279598, -19.907653361009256, -20.38651028791329, -20.84640448051192, -21.28808686308365, -21.71227862330554, -22.11967238982264, -22.510933363185664, -22.886700402003513, -23.247587066084172, -23.59418261826724, -23.92705298658386, -24.24674168831514, -24.553770717457855, -24.84864139704652, -25.131835197723483, -25.403814523893637, -25.665023468747446, -25.915888539385044, -26.156819353225394, -26.388209306837673, -26.610436218286896, -26.823862944042737, -27.028837971458646, -27.225695987788885, -27.414758426672446, -27.596333992976213, -27.770719166854352, -27.93819868784692, -28.099046019808185, -28.253523797423778, -28.401884255045797, -28.544369638545987, -28.68121260085956, -28.812636581865526, -28.93885617322365, -29.060077468764, -29.17649840100094, -29.288309064321304, -29.395692025374174, -29.498822621169367, -29.597869245371058, -29.692993623254367, -29.784351075773486, -29.872090773172857, -29.956355978555216, -30.037284281804432, -30.115007824244977, -30.189653514404874, -30.261343235234442, -30.33019404311915, -30.39631835901163, -30.45982415199478, -30.52081511557578, -30.579390836998982, -30.635646959853815, -30.68967534024361, -30.741564196769964, -30.79139825457787, -30.839258883696594, -30.8852242319022, -30.929369352318872, -30.971766325967046, -31.012484379458755, -31.051589998032185, -31.089147034110113, -31.125216811559355, -31.159858225821594, -31.19312784007907, -31.225079977611934, -31.2557668104985, -31.285238444802758, -31.31354300238857, -31.340726699493985, -31.366833922194026])\n",
      "0.1 learning rate.  ([0, 1.2000000000000002, 2.16, 2.928, 3.5423999999999998, 4.03392, 4.427136, 4.7417088, 4.99336704, 5.194693632, 5.3557549056, 5.48460392448, 5.587683139584, 5.6701465116672, 5.73611720933376, 5.788893767467008, 5.831115013973607, 5.864892011178886, 5.891913608943108, 5.9135308871544865, 5.9308247097235895, 5.944659767778871, 5.955727814223097, 5.964582251378478, 5.971665801102782, 5.977332640882226, 5.981866112705781, 5.9854928901646245, 5.9883943121317, 5.99071544970536, 5.992572359764288, 5.99405788781143, 5.9952463102491445, 5.996197048199315, 5.9969576385594525, 5.997566110847562, 5.99805288867805, 5.99844231094244, 5.998753848753951, 5.999003079003161, 5.999202463202529, 5.999361970562023, 5.999489576449618, 5.999591661159695, 5.999673328927756, 5.999738663142205, 5.999790930513764, 5.999832744411011, 5.999866195528808, 5.9998929564230465, 5.999914365138437, 5.99993149211075, 5.9999451936886, 5.99995615495088, 5.999964923960704, 5.999971939168563, 5.99997755133485, 5.99998204106788, 5.999985632854304, 5.999988506283443, 5.999990805026754, 5.999992644021403, 5.999994115217122, 5.999995292173698, 5.999996233738958, 5.9999969869911665, 5.999997589592933, 5.999998071674346, 5.999998457339477, 5.999998765871582, 5.999999012697265, 5.999999210157812, 5.999999368126249, 5.999999494500999, 5.9999995956008, 5.999999676480639, 5.999999741184512, 5.99999979294761, 5.999999834358087, 5.99999986748647, 5.999999893989176, 5.99999991519134, 5.999999932153072, 5.9999999457224575, 5.999999956577966, 5.999999965262373, 5.999999972209898, 5.999999977767919, 5.9999999822143355, 5.999999985771469, 5.999999988617175, 5.999999990893739, 5.999999992714992, 5.9999999941719935, 5.999999995337594, 5.999999996270075, 5.99999999701606, 5.999999997612848, 5.999999998090279, 5.999999998472223, 5.999999998777779], [4, -8.96, -17.2544, -22.562816, -25.96020224, -28.134529433600004, -29.526098837504, -30.416703256002556, -30.986690083841637, -31.351481653658652, -31.58494825834154, -31.734366885338574, -31.829994806616696, -31.891196676234685, -31.93036587279019, -31.95543415858573, -31.97147786149487, -31.98174583135671, -31.98831733206829, -31.99252309252371, -31.995214779215175, -31.996937458697715, -31.998039973566534, -31.99874558308258, -31.999197173172846, -31.999486190830623, -31.999671162131605, -31.999789543764223, -31.999865308009106, -31.999913797125828, -31.99994483016053, -31.999964691302736, -31.99997740243375, -31.9999855375576, -31.999990744036864, -31.99999407618359, -31.9999962087575, -31.9999975736048, -31.999998447107068, -31.999999006148528, -31.99999936393506, -31.999999592918442, -31.999999739467796, -31.999999833259395, -31.99999989328601, -31.999999931703044, -31.999999956289955, -31.999999972025563, -31.99999998209637, -31.999999988541674, -31.999999992666673, -31.999999995306666, -31.999999996996266, -31.999999998077612, -31.999999998769667, -31.99999999921259, -31.999999999496055, -31.999999999677478, -31.999999999793587, -31.999999999867896, -31.999999999915453, -31.999999999945892, -31.999999999965368, -31.99999999997783, -31.999999999985818, -31.99999999999092, -31.999999999994188, -31.999999999996284, -31.99999999999762, -31.99999999999848, -31.999999999999027, -31.999999999999375, -31.999999999999602, -31.999999999999744, -31.99999999999983, -31.9999999999999, -31.99999999999993, -31.999999999999957, -31.99999999999997, -31.999999999999986, -31.999999999999986, -31.999999999999986, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0])\n",
      "0.5 learning rate.  ([0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0], [4, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0])\n",
      "0.9 learning rate.  ([0, 10.8, 2.1599999999999984, 9.072000000000003, 3.542399999999998, 7.966080000000002, 4.427135999999999, 7.2582912, 4.993367039999999, 6.805306368000001, 5.3557549056, 6.51539607552, 5.587683139584, 6.3298534883328, 5.73611720933376, 6.211106232532992, 5.831115013973607, 6.135107988821114, 5.891913608943108, 6.0864691128455135, 5.9308247097235895, 6.055340232221129, 5.955727814223097, 6.035417748621522, 5.971665801102782, 6.022667359117774, 5.981866112705781, 6.0145071098353755, 5.9883943121317, 6.00928455029464, 5.992572359764288, 6.00594211218857, 5.9952463102491445, 6.003802951800685, 5.9969576385594525, 6.002433889152438, 5.99805288867805, 6.00155768905756, 5.998753848753951, 6.000996920996839, 5.999202463202529, 6.000638029437977, 5.999489576449618, 6.000408338840305, 5.999673328927756, 6.000261336857795, 5.999790930513764, 6.000167255588989, 5.999866195528808, 6.0001070435769535, 5.999914365138437, 6.00006850788925, 5.9999451936886, 6.00004384504912, 5.999964923960704, 6.000028060831437, 5.99997755133485, 6.00001795893212, 5.999985632854304, 6.000011493716557, 5.999990805026754, 6.000007355978597, 5.999994115217122, 6.000004707826302, 5.999996233738958, 6.0000030130088335, 5.999997589592933, 6.000001928325654, 5.999998457339477, 6.000001234128418, 5.999999012697265, 6.000000789842188, 5.999999368126249, 6.000000505499001, 5.9999995956008, 6.000000323519361, 5.999999741184512, 6.00000020705239, 5.999999834358087, 6.00000013251353, 5.999999893989176, 6.00000008480866, 5.999999932153072, 6.0000000542775425, 5.999999956577966, 6.000000034737627, 5.999999972209898, 6.000000022232081, 5.9999999822143355, 6.000000014228531, 5.999999988617175, 6.000000009106261, 5.999999992714992, 6.0000000058280065, 5.999999995337594, 6.000000003729925, 5.99999999701606, 6.000000002387152, 5.999999998090279, 6.000000001527777, 5.999999998777779], [4, -8.95999999999998, -17.254399999999986, -22.562815999999984, -25.960202239999987, -28.1345294336, -29.526098837503994, -30.416703256002563, -30.986690083841637, -31.351481653658645, -31.58494825834154, -31.734366885338588, -31.829994806616696, -31.891196676234685, -31.93036587279019, -31.955434158585724, -31.97147786149487, -31.98174583135672, -31.98831733206829, -31.99252309252371, -31.995214779215175, -31.996937458697708, -31.998039973566534, -31.99874558308258, -31.999197173172846, -31.99948619083063, -31.999671162131605, -31.999789543764223, -31.999865308009106, -31.999913797125828, -31.99994483016053, -31.999964691302736, -31.99997740243375, -31.999985537557606, -31.999990744036864, -31.999994076183597, -31.9999962087575, -31.9999975736048, -31.999998447107068, -31.999999006148528, -31.99999936393506, -31.999999592918435, -31.999999739467796, -31.999999833259395, -31.99999989328601, -31.99999993170305, -31.999999956289955, -31.99999997202557, -31.99999998209637, -31.999999988541674, -31.999999992666673, -31.999999995306666, -31.999999996996266, -31.999999998077612, -31.999999998769667, -31.99999999921259, -31.999999999496055, -31.99999999967747, -31.999999999793587, -31.999999999867896, -31.999999999915453, -31.999999999945885, -31.999999999965368, -31.999999999977838, -31.999999999985818, -31.99999999999092, -31.999999999994188, -31.999999999996277, -31.99999999999762, -31.99999999999848, -31.999999999999027, -31.999999999999375, -31.999999999999602, -31.999999999999744, -31.99999999999983, -31.999999999999893, -31.99999999999993, -31.999999999999957, -31.99999999999997, -31.999999999999986, -31.999999999999986, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0, -32.0])\n"
     ]
    }
   ],
   "source": [
    "a = [0.01, 0.1, 0.5, 0.9]\n",
    "epochs = 100\n",
    "for i in range(len(a)): \n",
    "    results = gradient_descent(function2, derivative2, 0, a[i], 100)\n",
    "    print(a[i], \"learning rate. \", results)\n",
    "    print(\"Final x:\")\n",
    "    print(\"Final f(x)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2592859a",
   "metadata": {},
   "source": [
    "Questions to answer:\n",
    "\n",
    "    Which learning rate converges fastest?\n",
    "        We can see an instantaneous snap to 6.0 from alpha=0.5.\n",
    "    What happens with α = 0.9? Why?\n",
    "        It's jumping past the correct value either size. With such a large training rate, there is too much chance of jumping past the correct answer with the modificaitons to x.\n",
    "    What would you expect with α = 1.1? (Don't implement, just reason about it)\n",
    "        It would never converge since it would always just balloon the x value like crazy and it would be worthless.\n",
    "    How many iterations did it take to get within 0.01 of the true minimum with α = 0.1\n",
    "        30 epochs, a very slow rate compared to the anomaly of 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f27c8077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([0, 1.2000000000000002, 2.16, 2.928, 3.5423999999999998, 4.03392, 4.427136, 4.7417088, 4.99336704, 5.194693632, 5.3557549056, 5.48460392448, 5.587683139584, 5.6701465116672, 5.73611720933376, 5.788893767467008, 5.831115013973607, 5.864892011178886, 5.891913608943108, 5.9135308871544865, 5.9308247097235895, 5.944659767778871, 5.955727814223097, 5.964582251378478, 5.971665801102782, 5.977332640882226, 5.981866112705781, 5.9854928901646245, 5.9883943121317, 5.99071544970536])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
